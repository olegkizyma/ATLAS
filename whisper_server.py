#!/usr/bin/env python3
"""
Local Whisper STT Server
Faster-Whisper based HTTP API for speech-to-text transcription.

Usage:
    python whisper_server.py [options]

Environment variables:
    WHISPER_MODEL: Model size (tiny/base/small/medium/large-v1/large-v2/large-v3), default: large-v3
    WHISPER_DEVICE: Device (cpu/cuda/auto/mps), default: cpu (mps maps to auto for Metal)
    WHISPER_COMPUTE_TYPE: Precision/quantization (int8, int8_float16, float16, int16, int8_bfloat16, etc.)
    WHISPER_HOST: Host to bind, default: 127.0.0.1
    WHISPER_PORT: Port to bind, default: 5000
    WHISPER_TEMP_DIR: Temporary directory for uploaded files, default: /tmp
"""

from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
import os
import tempfile
import shutil
import logging
from pathlib import Path

try:
    from faster_whisper import WhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False
    print("‚ö†Ô∏è  faster-whisper –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –í—Å—Ç–∞–Ω–æ–≤—ñ—Ç—å: pip install faster-whisper")

# –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è
MODEL_SIZE = os.getenv("WHISPER_MODEL", "large-v3")
_RAW_DEVICE = os.getenv("WHISPER_DEVICE", "cpu").lower()
if _RAW_DEVICE == 'mps':
    print("WHISPER_DEVICE=mps detected. Using device=auto for CTranslate2 (Metal support).")
    DEVICE = 'auto'
else:
    DEVICE = _RAW_DEVICE
COMPUTE_TYPE = (os.getenv("WHISPER_COMPUTE_TYPE") or ("float16" if DEVICE == 'cuda' else "int8")).lower()
HOST = os.getenv("WHISPER_HOST", "127.0.0.1")
PORT = int(os.getenv("WHISPER_PORT", "5000"))
TEMP_DIR = os.getenv("WHISPER_TEMP_DIR", tempfile.gettempdir())

# –ü—ñ–¥—Ç—Ä–∏–º—É–≤–∞–Ω—ñ —Ñ–æ—Ä–º–∞—Ç–∏ –∞—É–¥—ñ–æ
ALLOWED_EXTENSIONS = {
    'wav', 'mp3', 'mp4', 'm4a', 'aac', 'ogg', 'flac', 'webm', 'opus'
}

app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

# –õ–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª—ñ
whisper_model = None

def allowed_file(filename):
    """–ü–µ—Ä–µ–≤—ñ—Ä—è—î, —á–∏ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª—É."""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def init_whisper_model():
    """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î –º–æ–¥–µ–ª—å Whisper."""
    global whisper_model
    if not FASTER_WHISPER_AVAILABLE:
        return False
    
    try:
        logger.info(f"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –º–æ–¥–µ–ª—å Whisper: {MODEL_SIZE} –Ω–∞ –ø—Ä–∏—Å—Ç—Ä–æ—ó: {DEVICE} (compute_type={COMPUTE_TYPE})")
        whisper_model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE)
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å Whisper —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞")
        return True
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ Whisper: {e}")
        return False

@app.route('/health', methods=['GET'])
def health():
    """–ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Å—Ç–∞–Ω—É —Å–µ—Ä–≤–µ—Ä–∞."""
    return jsonify({
        'status': 'ok',
        'whisper_available': whisper_model is not None,
        'model': MODEL_SIZE,
    'device': DEVICE,
    'compute_type': COMPUTE_TYPE,
        'version': '1.0.0'
    })

@app.route('/transcribe', methods=['POST'])
def transcribe():
    """–û—Å–Ω–æ–≤–Ω–∏–π –µ–Ω–¥–ø–æ—ñ–Ω—Ç –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—ó –∞—É–¥—ñ–æ."""
    if not whisper_model:
        return jsonify({
            'error': 'Whisper model not available'
        }), 503

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Ñ–∞–π–ª—É
    if 'file' not in request.files:
        return jsonify({
            'error': 'No file uploaded'
        }), 400

    file = request.files['file']
    
    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ñ–∞–π–ª –Ω–µ –ø–æ—Ä–æ–∂–Ω—ñ–π
    if file.filename == '':
        return jsonify({
            'error': 'No file selected'
        }), 400

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è —Ñ–∞–π–ª—É
    if not allowed_file(file.filename):
        return jsonify({
            'error': f'Unsupported file format. Allowed: {", ".join(ALLOWED_EXTENSIONS)}'
        }), 400

    try:
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
        temp_file = tempfile.NamedTemporaryFile(
            dir=TEMP_DIR,
            suffix=f".{secure_filename(file.filename).rsplit('.', 1)[1].lower()}",
            delete=False
        )
        
        try:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ñ–∞–π–ª
            file.save(temp_file.name)
            logger.info(f"–§–∞–π–ª –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {temp_file.name}")
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—ó
            language = request.form.get('language') or 'uk'  # –¥–µ—Ñ–æ–ª—Ç: —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞
            beam_size = int(request.form.get('beam_size', 5))
            temperature = float(request.form.get('temperature', 0.0))
            
            # –í–∏–∫–æ–Ω—É—î–º–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—é
            logger.info(f"–†–æ–∑–ø–æ—á–∏–Ω–∞—é —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—é: {file.filename}")
            segments, info = whisper_model.transcribe(
                temp_file.name,
                beam_size=beam_size,
                temperature=temperature,
                language=language
            )
            
            # –ó–±–∏—Ä–∞—î–º–æ —Ç–µ–∫—Å—Ç –∑ —É—Å—ñ—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤
            text_segments = []
            full_text = ""
            
            for segment in segments:
                text_segments.append({
                    'start': segment.start,
                    'end': segment.end,
                    'text': segment.text
                })
                full_text += segment.text
            
            # –û—á–∏—â–∞—î–º–æ —Ç–µ–∫—Å—Ç
            full_text = full_text.strip()
            
            logger.info(f"‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(text_segments)} —Å–µ–≥–º–µ–Ω—Ç—ñ–≤, {len(full_text)} —Å–∏–º–≤–æ–ª—ñ–≤")
            
            return jsonify({
                'text': full_text,
                'language': info.language,
                'language_probability': info.language_probability,
                'duration': info.duration,
                'segments': text_segments
            })
            
        finally:
            # –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_file.name)
            except OSError:
                pass
                
    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü—ñ—ó: {e}")
        return jsonify({
            'error': f'Transcription failed: {str(e)}'
        }), 500

@app.route('/models', methods=['GET'])
def models():
    """–ü–æ–≤–µ—Ä—Ç–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –º–æ–¥–µ–ª—ñ."""
    available_models = [
        'tiny', 'tiny.en',
        'base', 'base.en', 
        'small', 'small.en',
        'medium', 'medium.en',
        'large-v1', 'large-v2', 'large-v3'
    ]
    
    return jsonify({
        'current_model': MODEL_SIZE,
        'available_models': available_models,
    'device': DEVICE,
    'compute_type': COMPUTE_TYPE
    })

@app.errorhandler(413)
def too_large(e):
    """–û–±—Ä–æ–±–∫–∞ –ø–æ–º–∏–ª–∫–∏ –∑–∞–≤–µ–ª–∏–∫–æ–≥–æ —Ñ–∞–π–ª—É."""
    return jsonify({
        'error': 'File too large. Maximum size: 50MB'
    }), 413

if __name__ == '__main__':
    print(f"üöÄ –ó–∞–ø—É—Å–∫ Whisper STT Server")
    print(f"üìç Host: {HOST}")
    print(f"üîå Port: {PORT}")
    print(f"üß† Model: {MODEL_SIZE}")
    print(f"üíª Device: {DEVICE}")
    print(f"üßÆ Compute type: {COMPUTE_TYPE}")
    print(f"üìÅ Temp dir: {TEMP_DIR}")
    
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É—î–º–æ –º–æ–¥–µ–ª—å
    if init_whisper_model():
        print("‚úÖ Whisper –≥–æ—Ç–æ–≤–∏–π –¥–æ —Ä–æ–±–æ—Ç–∏!")
        app.run(host=HOST, port=PORT, debug=False)
    else:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–≤–∞—Ç–∏ Whisper")
        exit(1)
