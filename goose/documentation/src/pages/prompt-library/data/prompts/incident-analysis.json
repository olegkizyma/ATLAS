{
  "id": "data-incident-analysis",
  "title": "Analyze PagerDuty Incident Patterns",
  "description": "Analyze incident data to identify patterns and opportunities for reducing alert noise",
  "category": "technical",
  "job": "engineering",
  "featured": true,
  "example_prompt": "Analyze the PagerDuty incident activity data to:\n1. Identify most frequent issues and their patterns\n2. Calculate incident frequencies and trends\n3. Determine if issues are ongoing or resolved\n4. Highlight top opportunities to reduce alert noise\n5. Generate recommendations for alert optimization\n\nThe analysis should focus on actionable insights to reduce unnecessary pages and improve on-call experience.",
  "example_result": "I've analyzed the PagerDuty incident data. Here's the comprehensive analysis:\n\n# Incident Analysis Report\n\n## 1. High-Frequency Issues\n\n```sql\nSELECT \n    incident_type,\n    COUNT(*) as occurrence_count,\n    AVG(resolution_time_minutes) as avg_resolution_time\nFROM incidents\nGROUP BY incident_type\nORDER BY occurrence_count DESC\nLIMIT 5;\n```\n\nTop 5 Most Frequent Issues:\n1. CPU Usage Spike (127 incidents)\n   - Peak times: 2-4 AM UTC\n   - Avg resolution: 12 minutes\n   - 92% auto-resolved\n\n2. Memory Warning (89 incidents)\n   - Consistent pattern on app-server-03\n   - Avg resolution: 8 minutes\n   - 95% auto-resolved\n\n3. API Response Time (67 incidents)\n   - Correlated with batch job schedule\n   - Avg resolution: 45 minutes\n   - 40% required manual intervention\n\n4. Disk Space Warning (52 incidents)\n   - Recurring on logging servers\n   - Avg resolution: 30 minutes\n   - 70% resolved by log rotation\n\n5. Connection Timeout (43 incidents)\n   - Database-related\n   - Avg resolution: 28 minutes\n   - 60% required manual intervention\n\n## 2. Temporal Analysis\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Incident frequency by hour\ndf['hour'] = pd.to_datetime(df['timestamp']).dt.hour\nhourlyCounts = df.groupby('hour')['incident_id'].count()\n\n# Plot distribution\nplt.figure(figsize=(12, 6))\nplt.bar(hourlyCounts.index, hourlyCounts.values)\nplt.title('Incident Distribution by Hour')\nplt.xlabel('Hour (UTC)')\nplt.ylabel('Number of Incidents')\n```\n\nKey Patterns:\n- 68% of alerts occur between 00:00-06:00 UTC\n- Wednesday shows highest incident rate\n- Month-end has 2.5x normal incident volume\n\n## 3. Issue Status Summary\n\nActive Issues:\n- Memory leaks on app-server-03\n  - Status: Under investigation\n  - Impact: Medium\n  - ETA: 48 hours\n\n- API response degradation\n  - Status: Partially mitigated\n  - Impact: Low\n  - ETA: 24 hours\n\nResolved Issues:\n- Database connection timeouts\n  - Resolution: Connection pool optimization\n  - Prevention: Added connection monitoring\n\n- Disk space warnings\n  - Resolution: Automated log rotation\n  - Prevention: Predictive cleanup jobs\n\n## 4. Alert Noise Reduction Opportunities\n\n1. High-Impact Opportunities:\n   ```\n   CPU Usage Alerts:\n   - Current: 127 alerts/month\n   - Potential: 15 alerts/month\n   - Reduction: 88%\n   - Effort: Low\n   ```\n\n2. Medium-Impact Opportunities:\n   ```\n   Memory Warnings:\n   - Current: 89 alerts/month\n   - Potential: 20 alerts/month\n   - Reduction: 78%\n   - Effort: Medium\n   ```\n\n## 5. Recommendations\n\n1. Immediate Actions:\n```yaml\n- Update CPU threshold:\n    from: 80% for 1 minute\n    to: 90% for 5 minutes\n    impact: -85% alerts\n\n- Implement memory alert aggregation:\n    window: 15 minutes\n    condition: Same host\n    impact: -70% alerts\n```\n\n2. Short-term Improvements:\n```yaml\n- Add context to API alerts:\n    - Include active connections\n    - Add recent deploy info\n    - Show batch job status\n\n- Enhance disk space monitoring:\n    - Predict fill rate\n    - Auto-trigger cleanup\n    - Weekly trend analysis\n```\n\n3. Long-term Projects:\n```yaml\n- Implement ML-based anomaly detection:\n    priority: High\n    effort: Large\n    benefits:\n      - 60% false positive reduction\n      - Automated threshold adjustment\n      - Pattern recognition\n\n- Develop unified alert dashboard:\n    priority: Medium\n    effort: Medium\n    features:\n      - Cross-service correlation\n      - Historical pattern view\n      - One-click suppression\n```\n\n## Implementation Plan\n\n1. Week 1-2:\n   - Adjust CPU thresholds\n   - Configure alert aggregation\n   - Update runbooks\n\n2. Week 3-4:\n   - Deploy enhanced monitoring\n   - Test new alert rules\n   - Train on-call team\n\n3. Month 2:\n   - Begin ML implementation\n   - Collect baseline metrics\n   - Develop dashboard\n\nExpected Outcomes:\n- 75% reduction in non-actionable alerts\n- 40% decrease in MTTR\n- 60% improvement in on-call satisfaction\n\nWould you like me to generate detailed configuration changes for any specific recommendation?",
  "extensions": [
    {
      "name": "Computer Controller",
      "command": "computercontroller",
      "is_builtin": true,
      "environmentVariables": []
    },
    {
      "name": "Developer",
      "command": "developer",
      "is_builtin": true,
      "environmentVariables": []
    }
  ]
}
